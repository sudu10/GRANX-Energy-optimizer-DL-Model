{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36131e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRANXModel(tf.keras.Model):    \n",
    "    def __init__(self, sequence_length, n_features, hidden_units=64, attention_heads=8, \n",
    "                 correction_rate=0.01, dropout_rate=0.2):\n",
    "        super(GRANXModel, self).__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_features = n_features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.correction_rate = correction_rate\n",
    "        self.conv1d_layers = [\n",
    "            layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
    "            layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(dropout_rate)\n",
    "        ]\n",
    "        self.temporal_layers = [\n",
    "            layers.GRU(hidden_units, return_sequences=True, dropout=dropout_rate),\n",
    "            layers.LSTM(hidden_units, return_sequences=True, dropout=dropout_rate),\n",
    "            layers.BatchNormalization()\n",
    "        ]\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=attention_heads, \n",
    "            key_dim=hidden_units,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.feature_dense = layers.Dense(hidden_units, activation='tanh')\n",
    "        self.feature_norm = layers.LayerNormalization()\n",
    "        self.correction_dense = layers.Dense(hidden_units, activation='linear')\n",
    "        self.output_projection = layers.Dense(1, activation='linear')\n",
    "        self.global_pool = layers.GlobalAveragePooling1D()\n",
    "        self.final_dropout = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def compute_feature_maps(self, x):\n",
    "        for layer in self.conv1d_layers:\n",
    "            x = layer(x)\n",
    "        for layer in self.temporal_layers:\n",
    "            x = layer(x)\n",
    "        M_t = self.feature_dense(x)\n",
    "        M_t = self.feature_norm(M_t)\n",
    "        return M_t\n",
    "    \n",
    "    def compute_attention_weights(self, M_t):\n",
    "        attended_features, attention_weights = self.attention(\n",
    "            M_t, M_t, return_attention_scores=True\n",
    "        )\n",
    "        alpha_t = tf.nn.softmax(tf.reduce_mean(attention_weights, axis=1), axis=-1)\n",
    "        return attended_features, alpha_t\n",
    "    \n",
    "    def compute_gradient_correction(self, weighted_features, y_true=None):\n",
    "        if y_true is not None and self.training:\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(weighted_features)\n",
    "                temp_pred = self.output_projection(self.global_pool(weighted_features))\n",
    "                loss = tf.reduce_mean(tf.square(temp_pred - y_true))\n",
    "            gradients = tape.gradient(loss, weighted_features)\n",
    "            correction = -self.correction_rate * gradients if gradients is not None else tf.zeros_like(weighted_features)\n",
    "        else:\n",
    "            correction = -self.correction_rate * self.correction_dense(weighted_features)\n",
    "        return correction\n",
    "    \n",
    "    def call(self, inputs, training=None, y_true=None):\n",
    "        M_t = self.compute_feature_maps(inputs)\n",
    "        attended_features, alpha_t = self.compute_attention_weights(M_t)\n",
    "        alpha_expanded = tf.expand_dims(alpha_t, axis=-1)\n",
    "        weighted_sum = weighted_sum = tf.reduce_sum(attended_features * tf.reduce_mean(alpha_expanded, axis=2), axis=1)\n",
    "        correction = self.compute_gradient_correction(attended_features, y_true)\n",
    "        corrected_features = attended_features - correction\n",
    "        pooled_features = self.global_pool(corrected_features)\n",
    "        pooled_features = self.final_dropout(pooled_features, training=training)\n",
    "        output = self.output_projection(pooled_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdfccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyDataProcessor:\n",
    "    \"\"\"Data processor for household energy consumption data\"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length=24, prediction_horizon=1):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.scaler = StandardScaler()\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        \n",
    "    def load_smart_meter_data(self, file_path):\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "            data = data.replace(-1, np.nan)\n",
    "            data = data.interpolate(method='linear')\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def aggregate_to_hourly(self, data):\n",
    "        if len(data) == 86400:\n",
    "            hourly_data = data.values.reshape(24, 3600, -1).mean(axis=1)\n",
    "            return pd.DataFrame(hourly_data, columns=data.columns)\n",
    "        return data\n",
    "    \n",
    "    def create_sequences(self, data, target_column='powerallphases'):\n",
    "        feature_columns = [\n",
    "            'powerallphases', 'powerl1', 'powerl2', 'powerl3',\n",
    "            'currentneutral', 'currentl1', 'currentl2', 'currentl3',\n",
    "            'voltagel1', 'voltagel2', 'voltagel3'\n",
    "        ]\n",
    "        available_columns = [col for col in feature_columns if col in data.columns]\n",
    "        features = data[available_columns].values\n",
    "        target = data[target_column].values\n",
    "        \n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        target_scaled = self.target_scaler.fit_transform(target.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        X, y = [], []\n",
    "        for i in range(len(features_scaled) - self.sequence_length - self.prediction_horizon + 1):\n",
    "            X.append(features_scaled[i:(i + self.sequence_length)])\n",
    "            y.append(target_scaled[i + self.sequence_length + self.prediction_horizon - 1])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def create_synthetic_data(self, n_samples=1000):\n",
    "        np.random.seed(42)\n",
    "        hours = np.arange(n_samples) % 24\n",
    "        days = np.arange(n_samples) // 24\n",
    "        base_consumption = (\n",
    "            500 +\n",
    "            200 * np.sin(2 * np.pi * hours / 24) +\n",
    "            100 * np.sin(2 * np.pi * days / 7) +\n",
    "            50 * np.random.normal(0, 1, n_samples)\n",
    "        )\n",
    "        features = {\n",
    "            'powerallphases': base_consumption,\n",
    "            'powerl1': base_consumption * 0.4 + np.random.normal(0, 20, n_samples),\n",
    "            'powerl2': base_consumption * 0.3 + np.random.normal(0, 15, n_samples),\n",
    "            'powerl3': base_consumption * 0.3 + np.random.normal(0, 15, n_samples),\n",
    "            'currentneutral': base_consumption * 0.02 + np.random.normal(0, 1, n_samples),\n",
    "            'currentl1': base_consumption * 0.008 + np.random.normal(0, 0.5, n_samples),\n",
    "            'currentl2': base_consumption * 0.006 + np.random.normal(0, 0.5, n_samples),\n",
    "            'currentl3': base_consumption * 0.006 + np.random.normal(0, 0.5, n_samples),\n",
    "            'voltagel1': 230 + np.random.normal(0, 5, n_samples),\n",
    "            'voltagel2': 230 + np.random.normal(0, 5, n_samples),\n",
    "            'voltagel3': 230 + np.random.normal(0, 5, n_samples),\n",
    "        }\n",
    "        return pd.DataFrame(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc96bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_granx_model(X_train, y_train, X_val, y_val, epochs=100, batch_size=32, learning_rate=0.001):\n",
    "    model = GRANXModel(\n",
    "        sequence_length=X_train.shape[1],\n",
    "        n_features=X_train.shape[2],\n",
    "        hidden_units=64,\n",
    "        attention_heads=8,\n",
    "        correction_rate=0.01\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=10, factor=0.5, min_lr=1e-6),\n",
    "        keras.callbacks.ModelCheckpoint('granx_model.h5', save_best_only=True)\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e70091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, target_scaler):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test_original = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    y_pred_original = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "    mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_original, y_pred_original)\n",
    "    \n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'y_true': y_test_original,\n",
    "        'y_pred': y_pred_original\n",
    "    }\n",
    "\n",
    "def plot_results(results, history):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 0].set_title('Model Loss'); axes[0, 0].legend()\n",
    "    \n",
    "    axes[0, 1].plot(history.history['mae'], label='Training MAE')\n",
    "    axes[0, 1].plot(history.history['val_mae'], label='Validation MAE')\n",
    "    axes[0, 1].set_title('Model MAE'); axes[0, 1].legend()\n",
    "    \n",
    "    axes[1, 0].scatter(results['y_true'], results['y_pred'], alpha=0.6)\n",
    "    axes[1, 0].plot([results['y_true'].min(), results['y_true'].max()],\n",
    "                    [results['y_true'].min(), results['y_true'].max()], 'r--')\n",
    "    axes[1, 0].set_title('Predictions vs Actual')\n",
    "    \n",
    "    n_points = min(200, len(results['y_true']))\n",
    "    axes[1, 1].plot(results['y_true'][:n_points], label='Actual')\n",
    "    axes[1, 1].plot(results['y_pred'][:n_points], label='Predicted')\n",
    "    axes[1, 1].set_title('Time Series Comparison')\n",
    "    axes[1, 1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"GRAN-X Energy Forecasting Model\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    processor = EnergyDataProcessor(sequence_length=24, prediction_horizon=1)\n",
    "    print(\"Creating synthetic energy consumption data...\")\n",
    "    data = processor.create_synthetic_data(n_samples=2000)\n",
    "    \n",
    "    print(\"Creating sequences for training...\")\n",
    "    X, y = processor.create_sequences(data)\n",
    "    \n",
    "    split_idx = int(0.7 * len(X))\n",
    "    val_idx = int(0.85 * len(X))\n",
    "    X_train, y_train = X[:split_idx], y[:split_idx]\n",
    "    X_val, y_val = X[split_idx:val_idx], y[split_idx:val_idx]\n",
    "    X_test, y_test = X[val_idx:], y[val_idx:]\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Validation set: {X_val.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "    \n",
    "    print(\"\\nTraining GRAN-X model...\")\n",
    "    model, history = train_granx_model(X_train, y_train, X_val, y_val, epochs=50, batch_size=32)\n",
    "    \n",
    "    print(\"\\nEvaluating model...\")\n",
    "    results = evaluate_model(model, X_test, y_test, processor.target_scaler)\n",
    "    \n",
    "    plot_results(results, history)\n",
    "    print(\"\\nGRAN-X model training completed successfully!\")\n",
    "    return model, results, processor\n",
    "\n",
    "model, results, processor = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "granx_env (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
